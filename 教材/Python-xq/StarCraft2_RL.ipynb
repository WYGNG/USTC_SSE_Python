{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练深度强化学习代理来玩“星际争霸 II (StarCraft II)”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欢迎来到本实验！开始之前，我们先来了解一下 Jupyter Notebook 上的一些指标。\n",
    "\n",
    "1.您可以使用浏览器打开 Jupyter Notebook 笔记本，不过具体内容则由在支持 AWS EC2 GPU 的实例上运行的交互式 iPython 内核进行流式传输。\n",
    "\n",
    "2.此笔记本由若干单元组成；单元中会包含您所能运行的代码，也能保存供您阅览的文本或图像。\n",
    "\n",
    "3.您可以通过单击菜单中的 ```Run```（运行）图标，或通过以下键盘快捷键 ```Shift-Enter```（运行并执行下一个）或 ```Ctrl-Enter```（运行并停留在当前单元）来执行代码单元。\n",
    "\n",
    "4.如要中止执行单元，请单击工具栏上的 ```Stop```（停止）按钮或前往 ```Kernel```（内核）菜单，并选择 ```Interrupt ```（中断）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一个标注单元 - 如果您运行我，我就会变为静态文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1  # i'm a code cell -- if you run me I'll perform some computations and show you their result below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习 (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习是机器学习的一个子领域，在此领域中，学习代理并非使用标注好的数据集，而是通过与环境交互来构建自己的经验数据集。最初，代理的动作是随机的，但当其偶然发现良好的行为时，便会受到环境的奖励，而且此奖励信号允许代理更新自身参数，以便日后能够将其所获奖励最大化。\n",
    "\n",
    "<img src=\"images/RL.jpg\" width=\"95%\"></img>\n",
    "\n",
    "自从 Google 的 DeepMind 使用 RL 和深度学习 (DL) 训练 AI 代理，使其仅通过像素来学习玩游戏，并以此掌握 Atari 游戏技能后，RL 最近广受追捧。自那时起，DeepMind 就已使用深度强化学习 (DRL) 成功挑战了围棋游戏。\n",
    "\n",
    "RL 要面临的下一个未攻克的艰巨挑战则是“星际争霸 II (StarCraft II)”- <a href=\"https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/\">了解详情</a>！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#“星际争霸 II (StarCraft II)”\n",
    "“星际争霸 II (StarCraft II)”(SC2) 是一款由 Blizzard 开发的实时策略游戏，可供免费畅玩（<a href='https://starcraft2.com/en-us/'>游戏官方网站链接</a>）。\n",
    "单击图片观看概述视频。\n",
    "\n",
    "<a href='https://www.youtube.com/watch?v=yaqeZ9Snt4E'> <img src=\"images/sc2.jpg\"></img> </a>\n",
    "    \n",
    "实时策略游戏需要玩家具备许多技能，包括：战略思维、准确或快速执行、信息收集或隐藏以及经济资源管理。在本实验中，我们将训练神经网络代理来玩迷你游戏，从而剥离并捕获全局游戏的一些基本技能。\n",
    "\n",
    "<a href=\"\"><img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/mini-games.gif\" width=\"90%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验组成部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验由以下部分组成：\n",
    "\n",
    "    \n",
    "<ul>\n",
    "<a href='#section1'>第 1 节</a>借助 DeepMind 的 Pysc2 API 以编程方式探索并掌握 SC2 的游戏技能\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section2'>第 2 节</a>了解如何使用奖励来塑造代理动作\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section3'>第 3 节</a>跟踪学习过程\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section4'>第 4 节</a> [混合与匹配] 在目标地图或新的陌生场景中部署训练后的代理\n",
    "    <br>&nbsp;<br>\n",
    "<a href='#section5'>第 5 节</a>在 Keras（TensorFlow 后端）构建或修改您自己的深度学习 SC2 代理\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\") # supress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# 第 1 节 - SC2 PyGame 客户端与 Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过与 Blizzard 的协同合作，DeepMind 已发布一款高级 Python API (<a href=\"https://github.com/deepmind/pysc2\">pysc2</a>)，我们可以用它来构建与“星际争霸 II (StarCraft II)”引擎进行交互的机器学习代理。\n",
    "\n",
    "下面我们将通过以下导入命令将此库加载到我们的笔记本中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们可以尝试使用 pysc2 来启动一个采用 py-game 渲染客户端的迷你游戏，该客户端将会生成一个低分辨率游戏视图（而我们的深度学习代理在玩游戏时将会用到此视图）。\n",
    "\n",
    "待加载的迷你游戏允许我们使用鼠标和键盘来控制 9 个人族太空<a href=\"http://us.battle.net/sc2/en/game/unit/marine\">陆战队员</a>组成的小队，以此对抗 4 只一组的虫族<a href=\"http://us.battle.net/sc2/en/game/unit/roach\">蟑螂</a>。\n",
    "\n",
    "通过运行以下命令，您将能启动带有“消灭蟑螂 (DefeatRoaches)”地图的 sc2 py-game 客户端。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pysc2.bin.play --map DefeatRoaches --max_game_steps 2000 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要查看此地图并在其上游玩，我们需要打开另一个选项卡，该选项卡将会运行一个远程图形桌面连接，而实验运行在AWS 云的实例中。\n",
    "\n",
    "\n",
    "###<center>单击 [noVNC Server](http://ec2-18-222-217-65.us-east-2.compute.amazonaws.com:6900/?password=vncpassword)（noVNC 服务器）查看 Pysc2 客户端。</center>\n",
    "\n",
    "接着，我们来启动游戏客户端，该客户端应会在 VNC 查看器中显示出来。注意，在 600 个游戏步骤结束之后，迷你游戏将会关闭，不过您可以再次重启（通过再次运行单元或继续进行余下内容）。注意：请使用 Internet Explorer 或 Microsoft Edge，因为其他浏览器可能不支持您与游戏交互。\n",
    "\n",
    "现在，您应能选择 VNC 查看器中显示的 GUI 游戏并与之交互。单击并拖动以选择（绿色的陆战队员圈），然后右键单击您想将所选单位移至的目标位置。\n",
    "\n",
    "\n",
    "<img src=\"images/marine_vs_roach.jpg\" width=\"50%\"></img><img src=\"images/marine_vs_roach_in_game.jpg\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想进一步探索，也可以试试其他迷你游戏，只需将以上单元中的“消灭蟑螂 (DefeatRoaches)”替换成其他的地图名即可。以下列出了一些可供您启动的迷你游戏。\n",
    "\n",
    "*“收集散落水晶 (CollectMineralShards)”\n",
    "*“消灭小狗和毒爆虫 (DefeatZerglingsAndBanelings)”\n",
    "*“寻找并消灭小狗 (FindAndDefeatZerglings)”\n",
    "*“寻路 (MoveToBeacon)”\n",
    "\n",
    "每个迷你游戏都试图教授 AI 一种掌控 SC2 所需的不同技能。代理必须学会什么技能才能掌控“消灭蟑螂 (DefeatRoaches)”？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>常见问题解答：</b>一些浏览器在 VNC 会话中不支持交互式游戏 - 尝试切换浏览器或禁用扩展程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - 以编程方式探索环境观测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将学习如何通过编程来创建 SC2 环境并与之交互。我们不直接使用鼠标和键盘，而是使用函数式 API 来探索 SC2 py-game 客户端产生的观测结果，并发出命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sc2_RL_environment.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['--'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "# define environment flags\n",
    "env_args = dict(\n",
    "        map_name='DefeatRoaches',\n",
    "        step_mul=1, # How many time steps occur between each action-decision. A step_mul of 1 means an agent can choose one action per frame.\n",
    "        game_steps_per_episode=0, # no limit- but each map has a built-in max number of steps and will terminate after reaching that.\n",
    "        screen_size_px = ( 64, 64 ), \n",
    "        minimap_size_px = ( 32, 32 ),\n",
    "        visualize = True,\n",
    "        score_index = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn the environment -- may take a minute to launch\n",
    "env = sc2_env.SC2Env(**env_args) # ** syntax implies variable number named arguments\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Pysc2 可用于 Linux、Windows 或 MacOS 系统。我们将在本实验中使用 Linux 实例，不过您也可下载代理的回放，然后在 Windows 或 Mac 设备上查看。\n",
    "\n",
    "现在，我们可以检阅 sc2 Linux 模拟器在每个游戏步骤后提供的原始观测结果样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测结果中包含众多有趣的数据元素。如果仔细观察，您应能找到当前时间步的奖励信息、累积分数以及许多有关游戏世界现状的其他细节，包括现有动作。由于我们刚刚踏入一个全新的游戏世界，因此这些值中有许多尚无具体信息可供参考；不过，待我们执行一些动作后，稍后就能再次返回查看这些值。下面我们就来演示一下如何获取它们的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'current reward: ', obs[0].reward )\n",
    "print( 'cumulative score: ', obs[0].observation['score_cumulative'][0])\n",
    "print( 'available data elements: ', obs[0].observation.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要特别关注一下屏幕的视觉层。\n",
    "\n",
    "这些可见的“特征层”代表各种游戏元素，如单位类型、单位所有者（玩家 1 与玩家 2）、单位生命值以及游戏状态的其他重要方面。每个特征层都作为网络的一个单独输入而提供，但由于这些特征层共用同一个空间参考框架，因此我们可将它们看作不同的通道或维度，共同构成对游戏状态的完整表示（即类似于红、绿、蓝三种颜色通道在照片中的呈现方式）\n",
    "\n",
    "<img src=\"images/feature_layers.png\" style=\"height:350px\">\n",
    "<center>\n",
    "**注意，这是在一场后期战斗中对非常活跃的屏幕进行的可视化。在玩迷你游戏时，我们不会看到如此复杂的场景。*\n",
    "</center>\n",
    "\n",
    "\n",
    "首先，我们将可视化所有数据通道及其名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numScreenLayers = obs[0].observation['screen'].shape[0] # 17\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "titles = ['heigh_map', 'visibility_map', 'creep', 'power', 'player_id', \n",
    "          'player_relative', 'unit_type', 'selected', \n",
    "          'unit_hit_points', 'unit_hit_points_ratio', \n",
    "          'unit_energy', 'unit_energy_ratio', \n",
    "          'unit_shields', 'unit_shields_ratio', \n",
    "          'unit_density', 'unit_density_aa', 'effects']\n",
    "[ [plt.subplot(5, 4, iScreenLayer+1), plt.imshow( obs[0].observation['screen'][iScreenLayer], aspect='equal'), \n",
    "       plt.title(titles[iScreenLayer]), plt.axis('off')] \n",
    " for iScreenLayer in range(numScreenLayers) ]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以放大其中一个数据通道以便更详细地查看。尝试更改屏幕索引并重新运行以下单元。您可<a href='https://github.com/deepmind/pysc2/blob/master/docs/environment.md#feature-layers'>在以下链接中</a>找到所有层的含义说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenIndex = 14 # choose a number between 0 and 16\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow( obs[0].observation['screen'][screenIndex])\n",
    "plt.title(titles[screenIndex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - 使用 API 发出动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经探索了 sc2 引擎产生的观测结果。下面我们来尝试执行一些可发送至引擎的动作，这样引擎就能作出响应并生成下一个步骤或观测结果。\n",
    "\n",
    "用于在“星际争霸 (StarCraft)”中发出动作的 python 函数式 API 能让我们生成整套动作，供人类玩家通过鼠标和键盘进行交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看看代理目前可发出哪些动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iAction in obs[0].observation['available_actions']:\n",
    "    print( actions.FUNCTIONS[iAction] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要了解如何将一系列鼠标和键盘操作转换为函数式 API 调用，可以参考以下动画图形。\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/Oriol-Fig-Anim-170809-Optimised-r03.gif\"></img>\n",
    "\n",
    "下方展示了我们可以使用函数式 API 发出的几个示例动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing\n",
    "doNothing = actions.FunctionCall( 0, [] ) \n",
    "\n",
    "# rectangle select and add to existing selection, rectangle from (0,0) to (31, 31)\n",
    "selectRectangle = actions.FunctionCall(3, [[1], [0,0], [31,31]])\n",
    "\n",
    "# select entire army\n",
    "selectEntireArmy = actions.FunctionCall(7, [[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将选择所有可用的陆军单位，并让它们攻击屏幕上的一个点（同时我们还要为后续 100 个游戏步骤重新发出此命令）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack screen location ( x=60, y=15 ) -- assumes a mini-game with at least 64x64 tiles\n",
    "attackScreen = actions.FunctionCall(12, [[0], [60, 15]])\n",
    "\n",
    "obs = env.step( [ selectEntireArmy ] )\n",
    "for i in range(100):\n",
    "    obs = env.step( [ attackScreen ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - [可选] 动作类型及构建您自己的动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节旨在详述有关 SC2 中编程动作的其他信息。每个动作都有一个特定的结构，这取决于它是否需要修饰符或空间参数。\n",
    "\n",
    "下面列出了几个可能会有的常见动作特征（非详尽列表）：\n",
    "\n",
    "```\n",
    "Type1: action.FunctionCall( functionID )\n",
    "Type2: action.FunctionCall( functionID, [ [ modifier ] ] ) # e.g., 'select_army'\n",
    "Type3: action.FunctionCall( functionID, [ [ modifier ], [x1, y1 ] ) # e.g., 'attack_screen'\n",
    "Type4: action.FunctionCall( functionID, [ [ modifier ], [x1, y1], [x2, y2] ) # e.g., 'select_rect'\n",
    "```\n",
    "<b>Type1</b> 属于简单型动作，例如“Stop_quick”(ID: 453) 或“HoldPosition_quick”(ID: 274)，这些动作都不需要任何修饰符或屏幕坐标。\n",
    "\n",
    "<b>Type2</b> 动作需要一个修饰符 - 在“select_army”(ID: 7) [选择所有存活的陆军单位] 的情况下，修饰符表明应将返回的陆军单位集添加至当前任意选择（例如一个农民单位）还是用其替换现有选择。\n",
    "\n",
    "<b>Type3</b> 动作需要一个修饰符和单个坐标 - 例如“Attack_screen”(ID: 12) 具有修饰符和单个坐标目标，其中修饰符表示是否应立即执行动作（或将动作添加至执行队列）。\n",
    "\n",
    "<b>Type4</b> 动作需要一个修饰符和两个屏幕坐标 - 例如“select_rect”(ID: 3) 需要一个修饰符和两个屏幕坐标，其中修饰符表示是否要替换、连结或从现有选择中删去新选择），屏幕坐标表示定义新选择的边框。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的 RL 代理只能使用此编程接口与游戏世界交互 - 您不妨尝试创建下方列出的有趣的动作序列！\n",
    "\n",
    "<b>实用提示：</b>如有需要，您可以随时将游戏状态<b>重置</b>为开始，具体可通过以下命令```obs = env.reset()```来执行此操作\n",
    "此外，我们还编写了一个 ```safe_action``` 功能（下方显示的几个单元），您可以借助此功能来帮助避免禁止执行的动作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - 随机动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已在下方为一个随机选择攻击点的代理编写了代码。您可能会发现，即使是随机代理也能偶然发现良好的行为；事实上，我们最初随机初始化代理参数时，正是据此来自动开启学习过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the action we chose can't be executed lets try to select our entire army, \n",
    "advance the game state, and try again.\n",
    "\"\"\"\n",
    "\n",
    "def safe_action ( actionToTake, obs ):\n",
    "    if actionToTake.function not in obs.observation['available_actions']:\n",
    "        print('unable to take selected action...lets try to fix things')\n",
    "        print('fix#1: select all army units')\n",
    "        obs = env.step( [ selectEntireArmy ] )\n",
    "        print('fix#2: perform no-op action')\n",
    "        obs = env.step( [ doNothing ] )\n",
    "        if actionToTake.function not in obs[0].observation['available_actions']:\n",
    "            print('!we are really in trouble...consider taking a different action')\n",
    "    else:\n",
    "        obs = env.step( [ actionToTake ] )\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = env.reset()\n",
    "nCycles = 1000\n",
    "for iCycles in range ( nCycles ):    \n",
    "    randomAttackScreen = actions.FunctionCall( 12, [[0], [np.random.randint(63), np.random.randint(63)]])\n",
    "    obs = safe_action ( randomAttackScreen, obs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# 第 2 节 - 奖励塑造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 DRL 训练的代理可以根据其训练环境（模拟器）和环境给予它们的奖励来学习行为。我们在相同的环境中训练了两个代理，游戏环境选取的是一张简易的迷你地图，并由两个蟑螂游戏包分离开来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - 通过 SC2 地图编辑器指定奖励\n",
    "我们使用地图编辑器来更改逻辑，以便控制环境所产生的奖励。如果您想构建自己的迷你游戏或修改现有的游戏，请下载并安装可免费游玩的“星际争霸 (StarCraft)”客户端，其中包括地图编辑器。然后，您可以通过修改地形或调整触发器中的逻辑来编辑地图。下方展示了默认设置的“消灭蟑螂 (DefeatRoaches)”地图的逻辑屏幕截图，其中每杀死一只蟑螂，都会增加 10 分（仅供参考：玩家一是我们的 RL 代理，玩家二是游戏中的脚本化 AI）。\n",
    "\n",
    "<img src=\"images/map_editor.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - 奖励修改和紧急行为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用 SC2 地图编辑器，构建了几种不同的“消灭蟑螂 (DefeatRoaches)”地图。在一个实例中，每流失一秒，我们就会施加惩罚，以此鼓励代理主动出击。经过长时间训练后，请执行以下单元查看此代理的记录。注意，上文中的代理一直在四处移动以寻找要击杀的敌人。通过更快速地发现及击杀所有蟑螂，此代理可获得更高的分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8eRFzXtBdwA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们制作了一张地图，其将针对小队成员的损失施加更严厉的惩罚。请执行以下单元查看这个不善主动出击的代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Opgktl6kLo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那位好斗的代理一直在四处寻找要击杀的蟑螂，而这位保守的代理却更乐意在角落里闲逛，以免损失任何陆战队员。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# 第 3 节 - 跟踪学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过查看代理所能获得的奖励，我们能够加深对它的了解。通过绘制这些训练时间或所见帧数的曲线，我们便能得到一条奖励曲线。我们使用 Tensorflow 的 Tensorboard 绘制了曲线。您可以在此 [链接](/tensorboard/) 中检阅许多不同代理的曲线。\n",
    "\n",
    "单击“toggle all runs”（触发所有运行进程），然后单击您想要探索的运行进程。稍后我们便会捕捉到许多指标，且 sc2 或 episode_score 会获得环境给予代理的分数。\n",
    "\n",
    "我们使用此开源 repo 来训练代理：https://github.com/simonmeister/pysc2-rl-agents.git 下图展示了在“消灭蟑螂 (DefeatRoaches)”迷你游戏中训练的代理的奖励曲线。\n",
    "<center>\n",
    "<img src=\"images/DefeatRoaches.PNG\" width=\"75%\"></img>\n",
    "</center>\n",
    "您可从学习曲线中看到，代理通常必须牺牲自己的分数方能学习一种新策略，最终取得更出色的游戏表现。下方提供了两个视频。第一个是只训练了一半的代理，第二个是完整训练后的代理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - 只训练了一半的代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tzPrtTXPTEA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，代理已经学会将注意力集中在蟑螂身上。不过，它仍在学习，并会花些时间点击地图的各个点。\n",
    "\n",
    "训练结束时，此代理效率惊人，并能轻松聚焦在蟑螂身上。运气好的话，代理将能斩获超高分！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IBUgp6097Q0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - 多个环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让代理同时在多个环境中运行可以加快其学习速度。我们使用 16 个环境同时训练其中一个代理，而对另一个代理则只使用一个环境。\n",
    "\n",
    "\n",
    "<img src=\"images/MoveToBeaconNenvs.PNG\" width=\"75%\"></img>\n",
    "在 16 个环境中训练的代理能够在 20 分钟内掌握“寻路 (MoveToBeacon)”迷你地图的游戏技能，而只使用 1 个环境训练的代理则需要 2 小时 20 分钟。\n",
    "\n",
    "“寻路 (MoveToBeacon)”是一个十分简单的迷你游戏，因此只在一个环境中学习的代理仍然可以掌握它。对于更复杂的环境而言，代理通常需要在多个环境中进行训练，以便学习掌握游戏所需的高级策略。\n",
    "\n",
    "浏览 [TensorBoard](/tensorboard/)，进一步探索奖励曲线。TensorBoard 也有一条为同时在 8 个环境中训练的代理绘制的奖励曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# 第 4 节 - 混合与匹配代理和环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，您可以选择一个代理，然后让该代理玩一个迷你游戏。您可以自由试验，探索不同的代理在不同环境下会有何表现。下方展示了一个可用代理列表以及可将其部署到的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可用代理\n",
    "\n",
    "* <b>DefeatRoaches</b> - 在“消灭蟑螂 (DefeatRoaches)”地图上受过完整训练的代理\n",
    "* <b>DefeatRoaches_half_trained</b> - 在“消灭蟑螂 (DefeatRoaches)”地图上受过部分训练的代理\n",
    "* <b>DefeatRoaches_singleRoundReset_conserveMarines_noTimePenalty_splitRoachPacks</b> - 竭力避免损失单位的代理\n",
    "* <b>DefeatRoaches_singleRoundReset_highTimePenalty_splitRoachPacks</b> - 必须尽快杀死对手的代理\n",
    "* <b>MoveToBeacon_n_envs_16</b> - 训练进行寻路的代理\n",
    "\n",
    "### 环境 [<a href=\"https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md\">详细描述</a>]\n",
    "<ul>\n",
    "“训练陆战队员 (BuildMarines)”<b>||</b>“收集散落水晶 (CollectMineralShards)”<b>||</b>“收集水晶和气 (CollectMineralsAndGas)”<br >\n",
    "“消灭蟑螂 (DefeatRoaches)”<b>||</b>“消灭小狗和毒爆虫 (DefeatZerglingsAndBanelings)”<b>||</b>“寻找并消灭小狗 (FindAndDefeatZerglings)”<br >\n",
    "“寻路 (MoveToBeacon)”\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如要加载代理，我们必须指定地图和检查点。输入下方的代理名称以查看模型检查点编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /notebooks/models/DefeatRoaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“检查点”文件会指向要加载的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"model_checkpoint_path: \\\"/notebooks/models/DefeatRoaches/model.ckpt-250000\\\"\" > /notebooks/models/DefeatRoaches/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成 10 次训练之后，回放将被写入磁盘。\n",
    "\n",
    "完成后，请单击工具栏上的 ```Stop```（停止）按钮或前往 ```Kernel```（内核）菜单，然后选择 ```Interrupt ```（中断），这样您便可执行更多单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /notebooks/pysc2-rl-agents/run.py \\\n",
    "  DefeatRoaches \\\n",
    "  --map DefeatRoaches \\\n",
    "  --max_windows 1 --gpu 0 --envs 1 \\\n",
    "  --step_mul 8 --steps_per_batch 16 \\\n",
    "  --vis --eval \\\n",
    "  --save_dir /notebooks/models \\\n",
    "  --summary_dir /notebooks/summary \\\n",
    "  --iters 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当生成回放后，您可以执行以下单元压缩文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update > /dev/null 2>&1 && apt-get install zip > /dev/null 2>&1 && zip /notebooks/replays.zip /notebooks/replays/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[单击此处下载回放。](replays.zip)\n",
    "\n",
    "如要使用本地客户端查看回放，请将回放文件置于 ```~/StarCraftII/replays```，并将迷你游戏置于 ```~/StarCraftII/Maps/mini_games``` 中。您可以在 [此处](https://github.com/deepmind/pysc2/tree/master/pysc2/maps/mini_games) 下载迷你游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# 第 5 节 - 构建并训练自己的代理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望您喜欢本实验，也希望您能学到一些关于 RL 和 SC2 的新知识。我们只是介绍了一些浅显知识，如果您有意深究，我们将乐意提供一些工具来进行更多探索。\n",
    "\n",
    "我们使用基于 TensorFlow 后端的 Keras 为一个自定义代理构建了训练工具，如下所示。我们的目标是为任何有意在 SC2 环境中试验 RL 想法的探究者提供一个易于使用的测试环境。\n",
    "\n",
    "此代码工具的结构如下：\n",
    "<ul>\n",
    "1 - 定义关键参数<br>\n",
    "2 - 创建多个 SC2 环境<br>\n",
    "3 - 加载代理代码<br>\n",
    "4 - 运行交互循环（使用 n-step 轨迹）并更新代理参数<br>\n",
    "</ul>\n",
    "从技术角度来说，该训练工具使用 n-step TD 进行学习，并可将观测的帧序列叠加在一起以探究动态变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require([\"base/js/dialog\"], function(dialog) {dialog.modal({title: 'Cleaning Up', body: 'We need to ensure that no other sc2 environments are active -- lets force a kernel restart', buttons: {'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }}});});\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os.path\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['initialize FLAGS for sc2 environments'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 - 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envParams = { \n",
    "    'simultaneousEnvironments': 4,\n",
    "    'nEnvironmentsToVisualize': 4,\n",
    "    'nTrajectorySteps': 16,\n",
    "    'nStackedFrames': 4,\n",
    "    'agentStepsPerEnvironmentStep': 4,\n",
    "    \n",
    "    'batchCheckpointInterval': 100,\n",
    "    'experimentDirectory': '/experiments',\n",
    "    'debugFlag': False,\n",
    "    \n",
    "    'screenResX': 64,\n",
    "    'screenResY': 64,\n",
    "    'screenResX_minimap': 32,\n",
    "    'screenResY_minimap': 32,\n",
    "    \n",
    "    'screenChannelsToKeep': [ 4, 5, 6, 7, 8, 9 ], # player_ID, player_relative, unit_type, selected, unit_hit_points, unit_hit_points_ratio\n",
    "    'screenChannelsRetained': 6,\n",
    "\n",
    "    'nonSpatialInputDimensions': 12, \n",
    "    'allowedActionIDs': [ 3, 12 ], # select_rect, attack_screen\n",
    "    'allowedActionIDRequiresModifier': [ 2, 1 ],            \n",
    "    'allowedActionIDRequiresLocation': [ 2, 1 ],       \n",
    "\n",
    "    'prunedActionSpaceSize': 2,\n",
    "    'actionArgumentSize': 4, \n",
    "\n",
    "    'nonVisualInputLength': 13,\n",
    "    \n",
    "    'futureDiscountRate': 1,   \n",
    "    'stepTypeFirst': 0,\n",
    "    'stepTypeMid': 1,\n",
    "    'stepTypeLast': 2,\n",
    "    \n",
    "    'entropyWeight': .25,\n",
    "    'policyWeight': 1,\n",
    "    'valueWeight': .5,\n",
    "    \n",
    "}\n",
    "\n",
    "# sanity check environment parameter definition\n",
    "assert ( envParams['prunedActionSpaceSize'] == len(envParams['allowedActionIDs']) \\\n",
    "            == len(envParams['allowedActionIDRequiresModifier']) \\\n",
    "            == len(envParams['allowedActionIDRequiresLocation']) )\n",
    "assert ( envParams['screenChannelsRetained'] == len(envParams['screenChannelsToKeep'] ) )\n",
    "\n",
    "\n",
    "assert ( envParams['nStackedFrames'] <= envParams['nTrajectorySteps'])\n",
    "\n",
    "sc2EnvLaunchParams = {\n",
    "    'map_name':'DefeatRoaches',\n",
    "    'step_mul': envParams['agentStepsPerEnvironmentStep'],\n",
    "    'game_steps_per_episode': 0, # no limit\n",
    "    'screen_size_px': ( envParams['screenResX'], envParams['screenResX']), \n",
    "    'minimap_size_px': ( envParams['screenResX_minimap'], envParams['screenResY_minimap']),\n",
    "    'visualize': False,\n",
    "    'score_index': None \n",
    "}\n",
    "sc2EnvLaunchParamsVis = sc2EnvLaunchParams.copy()\n",
    "sc2EnvLaunchParamsVis['visualize'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 - 启动多个环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将派生多个并行的工作进程，每个进程都自带内存空间且会运行一个 sc2 linux 客户端实例。每个工作进程都将通过一个双向通信管道连接到当前进程（笔记本内核），我们可通过此管道发送请求并与其中的 sc2 环境进行交互。\n",
    "\n",
    "<img src=\"images/multi_process.PNG\" style=\"width:40%\">\n",
    "\n",
    "对于每个管道而言，连接至主进程的末端称为局部末端，而连接至工作进程的末端则称为远程末端。我们使用管道末端在进程之间进行通信，通信时使用 ```.send()``` 命令进行传输并使用 ```.recv()``` 命令来监听回应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "# define the function running in each [forked] worker to parse communications between itself and the main process\n",
    "def sc2_remote_env_manager ( workerID, remotePipeEnd, sc2EnvLaunchParams ):\n",
    "    print('starting remote sc2 environment thread# ' + str(workerID))\n",
    "    env = sc2_env.SC2Env(**sc2EnvLaunchParams)\n",
    "    obs = env.reset()    \n",
    "    remotePipeEnd.send( ( 'launch complete', obs[0]) )\n",
    "    \n",
    "    while True:\n",
    "        command, arguments = remotePipeEnd.recv()\n",
    "        # take action and advance the game environment \n",
    "        if command == 'step': \n",
    "            obs = env.step( [ arguments ] )\n",
    "            assert( len(obs) == 1 )\n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        elif command == 'reset':\n",
    "            obs = env.reset()    \n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        # close the pipe and sc2 environment\n",
    "        elif command == 'close':\n",
    "            remotePipeEnd.send('closing')\n",
    "            remotePipeEnd.close()\n",
    "            break\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "localPipeEnds = [] # pipe-ends/communication channels used by the main process to communicate with workers\n",
    "remotePipeEnds = [] # pipe-ends/communication channels used by the workers to communicate with the main process\n",
    "processList = [] # list of worker threads\n",
    "\n",
    "# create two-way-communication channels [aka pipes] for remote workers and our main program\n",
    "# and spawn remote processes with remote pipe ends as an input argument\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    # create new pipe\n",
    "    localPipeEnd, remotePipeEnd = Pipe()\n",
    "    # store both ends \n",
    "    localPipeEnds += [ localPipeEnd ]\n",
    "    remotePipeEnds += [ remotePipeEnd ]\n",
    "    # spawn remote process and connect to remote pipe end\n",
    "    if iEnv < envParams['nEnvironmentsToVisualize']:\n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParamsVis) ) ]\n",
    "    else:        \n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParams) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {}\n",
    "for iEnv in range ( envParams['simultaneousEnvironments'] ):\n",
    "    obs[iEnv] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start remote workers, wait for each process to fully bring up the sc2 environment before creating the next\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    processList[iEnv].start()\n",
    "    while not localPipeEnds[iEnv].poll():\n",
    "        time.sleep(1)\n",
    "    msg, obs[iEnv] = localPipeEnds[iEnv].recv()\n",
    "    print ( msg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>单击 [noVNC Server](http://ec2-18-222-217-65.us-east-2.compute.amazonaws.com:6900/?password=vncpassword)（noVNC 服务器）查看环境。</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 - 加载代理代码\n",
    "\n",
    "以下显示了 [一个卷积代理的代码](../edit/convolutional_agent.py) - 您可以随时试验此代码并作出改进！\n",
    "此代理由基础版 [sc2_agent](../edit/convolutional_agent.py) 衍生而来 - 您可将此代理视作骨架，将卷积代理视作实际进行特征提取的大脑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' if using python3 uncomment -> ''' # from importlib import reload \n",
    "import convolutional_agent as sc2RL\n",
    "reload(sc2RL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sc2RL.ConvAgent( envParams )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environments\n",
    "for iEnv in range( envParams['simultaneousEnvironments'] ):\n",
    "    localPipeEnds[iEnv].send ( ( 'reset', [] ) )\n",
    "    obs[iEnv] = localPipeEnds[iEnv].recv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 - 行动 - 观察 - 学习 [循环]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RL_diagram.PNG\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "trainBatches = 15 # change to a large number or replace first for loop with a 'while True:' loop\n",
    "\n",
    "for iTrajectory in range(trainBatches):\n",
    "    for iStep in range ( envParams['nTrajectorySteps'] + 1):        \n",
    "        ''' batch predict model outputs on current inputs ''' # [ 1 timestep in multiple environments ]\n",
    "        batchModelOutputs = agent.batch_predict ( np.squeeze( agent.nEnvOneStepBatch, axis=1),\n",
    "                                                      np.squeeze( agent.nEnvOneStepBatchNonSpatial, axis=1)  )\n",
    "        \n",
    "        ''' update trajectory rewards and value estimates '''\n",
    "        agent.rewards[:, iStep ] = [ obs[iEnv].reward for iEnv in list(obs.keys()) ]\n",
    "        agent.valuePredictions[:, iStep] = batchModelOutputs[:, agent.policyInds['value']]\n",
    "        \n",
    "        if iStep != envParams['nTrajectorySteps']: # don't advance when in the final step -- use it to bootstrap loss computation\n",
    "            ''' sample and mask '''\n",
    "            sc2functionCalls, actionIDs, actionArguments = \\\n",
    "                agent.sample_and_mask ( obs, batchModelOutputs )\n",
    "\n",
    "            ''' compute partial loss terms ''' # logProbs and masked policy entropy\n",
    "            agent.inplace_update_logProbs_and_entropy ( iStep, batchModelOutputs )\n",
    "\n",
    "            ''' step i.e. apply selected action in each environment ''' # and get new observations\n",
    "            obs, _ = agent.step_in_envs ( obs, localPipeEnds, sc2functionCalls, actionIDs )\n",
    "\n",
    "            ''' compile the spatial and non-spatial trajectory observations ''' # needed for batch train update\n",
    "            agent.inplace_update_trajectory_observations( iStep, obs )\n",
    "        \n",
    "    ''' finished generating a trajectory -- compute nStep returns, advantages, and cumulative loss '''\n",
    "    agent.compute_loss ()\n",
    "    agent.train()\n",
    "    \n",
    "    print( 'trajectory# ' + str(trainBatches) + ' avg step reward: ' + str(np.mean( np.mean( agent.rewards ))))\n",
    "    if (trainBatches + 1) % envParams['batchCheckpointInterval'] == 0:\n",
    "        agent.model_checkpoint()            \n",
    "    trainBatches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结束语\n",
    "\n",
    "感谢您参与本实验。如果您想进一步探索强化学习的缤纷世界，请随时探索以下资源：\n",
    "\n",
    "Udacity 深度学习强化课程\n",
    "https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893\n",
    "\n",
    "DeepMind 对深度学习强化方法的概述\n",
    "https://deepmind.com/blog/deep-reinforcement-learning/\n",
    "\n",
    "探讨策略梯度网络的博文\n",
    "https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
